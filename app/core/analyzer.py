"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MODULE: app/core/analyzer.py - VERSION CORRIGÃ‰E
Auteur: Stage R&D - IMT Nord Europe
Fonction: Analyses Statistiques AvancÃ©es pour Formulations BÃ©ton
Version: 2.1.0 - Compatible avec predictor.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Any
from enum import Enum

import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import StandardScaler
import logging

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPORT CORRIGÃ‰ : Import de la fonction de prÃ©diction standard
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
from app.core.predictor import predict_concrete_properties

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLASSES DE DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SensitivityResult:
    """RÃ©sultat d'une analyse de sensibilitÃ©."""
    
    parameter_name: str
    """Nom du paramÃ¨tre variÃ©"""
    
    baseline_value: float
    """Valeur de rÃ©fÃ©rence"""
    
    variation_range: Tuple[float, float]
    """Plage de variation testÃ©e (min, max)"""
    
    impacts: Dict[str, List[float]]
    """Impact sur chaque cible {cible: [valeurs]}"""
    
    elasticities: Dict[str, float]
    """Ã‰lasticitÃ©s calculÃ©es {cible: Ã©lasticitÃ©}"""


@dataclass
class CorrelationAnalysis:
    """RÃ©sultat d'une analyse de corrÃ©lation."""
    
    correlation_matrix: pd.DataFrame
    """Matrice de corrÃ©lation"""
    
    significant_pairs: List[Tuple[str, str, float]]
    """Paires significativement corrÃ©lÃ©es (var1, var2, r)"""
    
    vif_scores: Dict[str, float]
    """Variance Inflation Factor (multicolinÃ©aritÃ©)"""


@dataclass
class ConfidenceInterval:
    """Intervalle de confiance d'une prÃ©diction."""
    
    mean: float
    """Valeur moyenne prÃ©dite"""
    
    lower_bound: float
    """Borne infÃ©rieure (percentile 2.5%)"""
    
    upper_bound: float
    """Borne supÃ©rieure (percentile 97.5%)"""
    
    confidence_level: float = 0.95
    """Niveau de confiance (dÃ©faut 95%)"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANALYSEUR PRINCIPAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ConcreteAnalyzer:
    """
    Analyseur statistique avancÃ© pour formulations bÃ©ton.
    
    Fournit des mÃ©thodes pour comprendre l'influence des paramÃ¨tres,
    dÃ©tecter les corrÃ©lations, et quantifier l'incertitude des prÃ©dictions.
    """
    
    def __init__(self):
        """Initialise l'analyseur."""
        self.scaler = StandardScaler()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ANALYSE DE SENSIBILITÃ‰ (CORRIGÃ‰E)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def sensitivity_analysis(
        self,
        baseline_formulation: Dict[str, float],
        parameter: str,
        feature_list: List[str],  # AJOUTÃ‰ : Liste des features pour alignement
        predictor: Any,             # AJOUTÃ‰ : Le modÃ¨le ML
        variation_percent: float = 20.0,
        n_points: int = 20
    ) -> SensitivityResult:
        """
        Analyse l'impact de la variation d'un paramÃ¨tre sur les cibles.
        
        Args:
            baseline_formulation: Formulation de rÃ©fÃ©rence
            parameter: Nom du paramÃ¨tre Ã  faire varier (ex: "Ciment")
            feature_list: Liste ordonnÃ©e des features (pour predictor.py)
            predictor: Instance du modÃ¨le ML
            variation_percent: % de variation autour de la baseline
            n_points: Nombre de points de test
        
        Returns:
            SensitivityResult avec impacts dÃ©taillÃ©s
        """
        if parameter not in baseline_formulation:
            raise ValueError(f"ParamÃ¨tre '{parameter}' absent de la formulation")
        
        baseline_value = baseline_formulation[parameter]
        
        # DÃ©finir la plage de variation
        delta = baseline_value * (variation_percent / 100)
        min_value = max(0, baseline_value - delta)
        max_value = baseline_value + delta
        
        # GÃ©nÃ©rer les valeurs de test
        test_values = np.linspace(min_value, max_value, n_points)
        
        # Stocker les impacts sur chaque cible
        impacts = {
            "Resistance": [],
            "Diffusion_Cl": [],
            "Carbonatation": []
        }
        
        # Simulation : variation du paramÃ¨tre
        for value in test_values:
            # CrÃ©er formulation modifiÃ©e
            modified_formulation = baseline_formulation.copy()
            modified_formulation[parameter] = value
            
            # PRÃ‰DICTION CORRIGÃ‰E : Utilisation de la fonction standard
            if predictor is not None:
                try:
                    predictions = predict_concrete_properties(
                        composition=modified_formulation,
                        model=predictor,
                        feature_list=feature_list
                    )
                    # Extraction des valeurs
                    impacts["Resistance"].append(predictions["Resistance"])
                    impacts["Diffusion_Cl"].append(predictions["Diffusion_Cl"])
                    impacts["Carbonatation"].append(predictions["Carbonatation"])
                except Exception as e:
                    logger.error(f"Erreur prÃ©diction sensibilitÃ©: {e}")
                    # Valeurs par dÃ©faut en cas d'erreur
                    impacts["Resistance"].append(30)
                    impacts["Diffusion_Cl"].append(10)
                    impacts["Carbonatation"].append(15)
            else:
                # Mode simulation (sans modÃ¨le rÃ©el)
                if parameter == "Ciment":
                    impacts["Resistance"].append(25 + value * 0.05)
                elif parameter == "Eau":
                    impacts["Resistance"].append(40 - value * 0.05)
                else:
                    impacts["Resistance"].append(30)
                impacts["Diffusion_Cl"].append(10)
                impacts["Carbonatation"].append(15)
        
        # Calculer les Ã©lasticitÃ©s (sensibilitÃ© relative)
        elasticities = self._calculate_elasticity(
            test_values,
            impacts,
            baseline_value
        )
        
        logger.info(
            f"Analyse sensibilitÃ© {parameter} : "
            f"Ã‰lasticitÃ© RÃ©sistance = {elasticities.get('Resistance', 0):.3f}"
        )
        
        return SensitivityResult(
            parameter_name=parameter,
            baseline_value=baseline_value,
            variation_range=(min_value, max_value),
            impacts=impacts,
            elasticities=elasticities
        )
    
    def _calculate_elasticity(
        self,
        param_values: np.ndarray,
        impacts: Dict[str, List[float]],
        baseline_param: float
    ) -> Dict[str, float]:
        """Calcule l'Ã©lasticitÃ© : (Î”Y/Y) / (Î”X/X)"""
        elasticities = {}
        
        for target_name, target_values in impacts.items():
            slope, _, _, _, _ = stats.linregress(param_values, target_values)
            
            # Ã‰lasticitÃ© au point baseline
            baseline_idx = len(param_values) // 2
            baseline_target = target_values[baseline_idx]
            
            if baseline_target != 0 and baseline_param != 0:
                elasticity = slope * (baseline_param / baseline_target)
            else:
                elasticity = 0.0
            
            elasticities[target_name] = elasticity
        
        return elasticities
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # INTERVALLES DE CONFIANCE (CORRIGÃ‰)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def confidence_interval(
        self,
        formulation: Dict[str, float],
        feature_list: List[str],  # AJOUTÃ‰
        predictor: Any,             # AJOUTÃ‰
        n_bootstrap: int = 100,
        confidence_level: float = 0.95
    ) -> Dict[str, ConfidenceInterval]:
        """
        Calcule les intervalles de confiance des prÃ©dictions par bootstrap.
        
        Args:
            formulation: Composition bÃ©ton
            feature_list: Liste des features
            predictor: ModÃ¨le de prÃ©diction
            n_bootstrap: Nombre d'Ã©chantillons bootstrap
            confidence_level: Niveau de confiance
        
        Returns:
            {cible: ConfidenceInterval}
        """
        if predictor is None:
            raise RuntimeError("ModÃ¨le requis pour calculer les intervalles de confiance")
        
        # PrÃ©diction de rÃ©fÃ©rence
        baseline_pred = predict_concrete_properties(
            composition=formulation,
            model=predictor,
            feature_list=feature_list
        )
        
        # GÃ©nÃ©rer des variantes par perturbation
        bootstrap_predictions = {
            target: [] for target in ["Resistance", "Diffusion_Cl", "Carbonatation"]
        }
        
        for _ in range(n_bootstrap):
            # Perturbation alÃ©atoire (Â±5% de bruit gaussien)
            perturbed = {
                key: max(0, value * np.random.normal(1.0, 0.05))
                for key, value in formulation.items()
            }
            
            # PRÃ‰DICTION CORRIGÃ‰E
            try:
                pred = predict_concrete_properties(
                    composition=perturbed,
                    model=predictor,
                    feature_list=feature_list
                )
                
                # Stockage des rÃ©sultats
                if "Resistance" in pred:
                    bootstrap_predictions["Resistance"].append(pred["Resistance"])
                if "Diffusion_Cl" in pred:
                    bootstrap_predictions["Diffusion_Cl"].append(pred["Diffusion_Cl"])
                if "Carbonatation" in pred:
                    bootstrap_predictions["Carbonatation"].append(pred["Carbonatation"])
                    
            except Exception as e:
                logger.warning(f"Erreur bootstrap itÃ©ration: {e}")
                continue
        
        # Calcul des intervalles
        alpha = 1 - confidence_level
        intervals = {}
        
        for target, values in bootstrap_predictions.items():
            if not values:
                continue # Skip si pas de donnÃ©es
                
            values_array = np.array(values)
            
            intervals[target] = ConfidenceInterval(
                mean=np.mean(values_array),
                lower_bound=np.percentile(values_array, alpha/2 * 100),
                upper_bound=np.percentile(values_array, (1 - alpha/2) * 100),
                confidence_level=confidence_level
            )
        
        logger.info(
            f"Intervalles confiance calculÃ©s : {len(intervals)} cibles"
        )
        
        return intervals
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ANALYSE COMPARATIVE (CORRIGÃ‰E)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def compare_formulations(
        self,
        formulations: List[Dict[str, float]],
        names: List[str],
        feature_list: List[str],  # AJOUTÃ‰
        predictor: Any             # AJOUTÃ‰
    ) -> pd.DataFrame:
        """
        Compare plusieurs formulations cÃ´te Ã  cÃ´te.
        """
        if len(formulations) != len(names):
            raise ValueError("Nombre de formulations â‰  nombre de noms")
        
        results = []
        
        for i, (formulation, name) in enumerate(zip(formulations, names)):
            # PRÃ‰DICTION CORRIGÃ‰E
            if predictor:
                try:
                    pred = predict_concrete_properties(
                        composition=formulation,
                        model=predictor,
                        feature_list=feature_list
                    )
                    
                    # Compilation rÃ©sultats
                    result = {
                        "Nom": name,
                        **formulation, # On ajoute toute la composition
                        **pred      # On ajoute les prÃ©dictions
                    }
                    results.append(result)
                    
                except Exception as e:
                    logger.error(f"Erreur prÃ©diction formulation {name}: {e}")
            else:
                # Mode simulation (test sans modÃ¨le)
                result = {
                    "Nom": name,
                    **formulation,
                    "Resistance": 30.0,
                    "Diffusion_Cl": 10.0,
                    "Carbonatation": 15.0,
                    "Ratio_E_L": formulation["Eau"] / (
                        formulation["Ciment"] + formulation.get("Laitier", 0) + 1e-5
                    ),
                    "Liant_Total": (
                        formulation["Ciment"] + 
                        formulation.get("Laitier", 0) + 
                        formulation.get("CendresVolantes", 0)
                    )
                }
                results.append(result)
        
        logger.info(f"Comparaison de {len(results)} formulations")
        
        return pd.DataFrame(results)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AUTRES MÃ‰THODES (InchangÃ©es, fournies pour complÃ©tude)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def correlation_analysis(
        self,
        data: pd.DataFrame,
        threshold: float = 0.7
    ) -> CorrelationAnalysis:
        """Analyse les corrÃ©lations entre variables."""
        corr_matrix = data.corr()
        significant_pairs = []
        
        for i in range(len(corr_matrix.columns)):
            for j in range(i + 1, len(corr_matrix.columns)):
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                r = corr_matrix.iloc[i, j]
                
                if abs(r) >= threshold:
                    significant_pairs.append((var1, var2, r))
        
        significant_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
        vif_scores = self._calculate_vif(data)
        
        return CorrelationAnalysis(
            correlation_matrix=corr_matrix,
            significant_pairs=significant_pairs,
            vif_scores=vif_scores
        )
    
    def _calculate_vif(self, data: pd.DataFrame) -> Dict[str, float]:
        """Calcule le VIF pour dÃ©tecter la multicolinÃ©aritÃ©."""
        from sklearn.linear_model import LinearRegression
        vif_scores = {}
        
        for i, col in enumerate(data.columns):
            X = data.drop(columns=[col]).values
            y = data[col].values
            
            model = LinearRegression()
            model.fit(X, y)
            r_squared = model.score(X, y)
            
            if r_squared < 0.9999:
                vif = 1 / (1 - r_squared)
            else:
                vif = float('inf')
            
            vif_scores[col] = vif
        
        return vif_scores
    
    def detect_outliers(
        self,
        data: pd.DataFrame,
        method: str = "zscore",
        threshold: float = 3.0
    ) -> pd.DataFrame:
        """DÃ©tecte les formulations aberrantes."""
        result = data.copy()
        result["is_outlier"] = False
        
        if method == "zscore":
            z_scores = np.abs(stats.zscore(data.select_dtypes(include=[np.number])))
            outlier_mask = (z_scores > threshold).any(axis=1)
            result["is_outlier"] = outlier_mask
        elif method == "iqr":
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            
            outlier_mask = (
                (data < (Q1 - 1.5 * IQR)) | 
                (data > (Q3 + 1.5 * IQR))
            ).any(axis=1)
            result["is_outlier"] = outlier_mask
        
        n_outliers = result["is_outlier"].sum()
        logger.info(f"DÃ©tection outliers : {n_outliers} dÃ©tectÃ©s")
        
        return result
    
    def performance_score(
        self,
        predictions: Dict[str, float],
        weights: Optional[Dict[str, float]] = None
    ) -> float:
        """Calcule un score de performance global multi-critÃ¨res."""
        if weights is None:
            weights = {
                "Resistance": 0.40,
                "Diffusion_Cl": 0.30,
                "Carbonatation": 0.30
            }
        
        # Normalisation (0-100)
        normalized = {}
        
        # RÃ©sistance
        res = predictions["Resistance"]
        normalized["Resistance"] = np.clip((res - 10) / (60 - 10) * 100, 0, 100)
        
        # Diffusion Cl
        diff_cl = predictions["Diffusion_Cl"]
        normalized["Diffusion_Cl"] = np.clip((20 - diff_cl) / (20 - 2) * 100, 0, 100)
        
        # Carbonatation
        carb = predictions["Carbonatation"]
        normalized["Carbonatation"] = np.clip((40 - carb) / (40 - 5) * 100, 0, 100)
        
        # Score pondÃ©rÃ©
        score = sum(
            normalized[key] * weights[key]
            for key in weights.keys()
        )
        
        return round(score, 1)
    
    def robustness_analysis(
        self,
        formulation: Dict[str, float],
        feature_list: List[str],  # AJOUTÃ‰
        predictor: Any,             # AJOUTÃ‰
        n_simulations: int = 50
    ) -> Dict[str, float]:
        """Analyse la robustesse de la formulation."""
        results = {
            "Resistance": [],
            "Diffusion_Cl": [],
            "Carbonatation": []
        }
        
        for _ in range(n_simulations):
            # Perturbation Â±3%
            perturbed = {
                key: max(0, value * np.random.uniform(0.97, 1.03))
                for key, value in formulation.items()
            }
            
            # PRÃ‰DICTION CORRIGÃ‰E
            try:
                pred = predict_concrete_properties(
                    composition=perturbed,
                    model=predictor,
                    feature_list=feature_list
                )
                
                results["Resistance"].append(pred["Resistance"])
                results["Diffusion_Cl"].append(pred["Diffusion_Cl"])
                results["Carbonatation"].append(pred["Carbonatation"])
            except Exception:
                continue
        
        # Calcul coefficients de variation (CV)
        cv_resistance = (
            np.std(results["Resistance"]) / 
            (np.mean(results["Resistance"]) + 1e-10) * 100
        )
        cv_diffusion = (
            np.std(results["Diffusion_Cl"]) / 
            (np.mean(results["Diffusion_Cl"]) + 1e-10) * 100
        )
        cv_carbonatation = (
            np.std(results["Carbonatation"]) / 
            (np.mean(results["Carbonatation"]) + 1e-10) * 100
        )
        
        # Score de fiabilitÃ©
        mean_cv = (cv_resistance + cv_diffusion + cv_carbonatation) / 3
        reliability_score = max(0, 100 - mean_cv * 10)
        
        logger.info(
            f"Analyse robustesse : Score fiabilitÃ© = {reliability_score:.1f}/100"
        )
        
        return {
            "cv_resistance": round(cv_resistance, 2),
            "cv_diffusion_cl": round(cv_diffusion, 2),
            "cv_carbonatation": round(cv_carbonatation, 2),
            "reliability_score": round(reliability_score, 1)
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FONCTIONS UTILITAIRES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def quick_sensitivity(
    baseline: Dict[str, float],
    parameter: str,
    feature_list: List[str], # AJOUTÃ‰
    predictor: Any              # AJOUTÃ‰
) -> SensitivityResult:
    """
    Analyse rapide de sensibilitÃ© (version simplifiÃ©e).
    """
    analyzer = ConcreteAnalyzer()
    return analyzer.sensitivity_analysis(
        baseline_formulation=baseline,
        parameter=parameter,
        feature_list=feature_list,
        predictor=predictor,
        variation_percent=20,
        n_points=15
    )


def format_sensitivity_report(result: SensitivityResult) -> str:
    """Formate un rapport de sensibilitÃ© en Markdown."""
    lines = []
    lines.append(f"# ğŸ“Š ANALYSE DE SENSIBILITÃ‰ : {result.parameter_name}")
    lines.append("")
    lines.append(f"**Valeur baseline :** {result.baseline_value:.2f}")
    lines.append(f"**Plage testÃ©e :** {result.variation_range[0]:.1f} - {result.variation_range[1]:.1f}")
    lines.append("")
    
    lines.append("## ğŸ¯ Ã‰lasticitÃ©s")
    for target, elasticity in result.elasticities.items():
        interpretation = ""
        if abs(elasticity) > 1:
            interpretation = "(TrÃ¨s sensible)"
        elif abs(elasticity) > 0.5:
            interpretation = "(Sensible)"
        else:
            interpretation = "(Peu sensible)"
        
        lines.append(f"- **{target}** : {elasticity:.3f} {interpretation}")
    
    lines.append("")
    lines.append("### ğŸ“– InterprÃ©tation")
    lines.append(
        "Ã‰lasticitÃ© = variation relative de la sortie / variation relative de l'entrÃ©e"
    )
    lines.append(
        "Ex : Ã‰lasticitÃ© = 0.8 â†’ +10% du paramÃ¨tre â†’ +8% de la cible"
    )
    
    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXPORTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

__all__ = [
    "ConcreteAnalyzer",
    "SensitivityResult",
    "CorrelationAnalysis",
    "ConfidenceInterval",
    "quick_sensitivity",
    "format_sensitivity_report"
]